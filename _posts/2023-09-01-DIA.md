---
title: Drone Image Analysis from field data using a Micasense Multispectral sensor
tags:
  - Drone
  - Multispectral
  - En
images01:
  - /fhc-www/_posts/figures/20230901/M1.png
  - /fhc-www/_posts/figures/20230901/M2.png
  - /fhc-www/_posts/figures/20230901/M3.png
  - /fhc-www/_posts/figures/20230901/M4.png
  - /fhc-www/_posts/figures/20230901/M5.png
---


This tutorial contains instructions of how to perform Drone Image analysis for a multispectral sensor. It includes our tips of how to make it simple for a new user. Here we cover the concepts related to mission planning, stitching and plot extraction.

<!--more-->

# Mission

## Before flying: Planning!

You can agree that capturing images with a drone is a fun activity! When we want to capture images from multiple flights and use it for research, is important that we follow some good standards. Here, we want to share how we performed our research in the summer of 2023. We used a DJI Matrice 200 V2 equipped with a Micasense Altum! 

We performed our flights over a winter wheat trial. As any of our drones was equipped with RTK, we first put ground control points (GCPs) in the field and surveyed them using a Multi-band RTK GNSS receiver. MS [Frank Dougher](https://landresources.montana.edu/directory/faculty/2324972/frank-dougher) helped us to survey the points at the field. We used at least five GCPs, four located at the corners of the field and a central GCP.  According to [Pugh *et al* (2021)](https://acsess.onlinelibrary.wiley.com/doi/full/10.1002/ppj2.20026), four GCPs located at the corners of the field should be enough.


## Understanding your sensor and flying

Micasense multispectral sensors are commercialized by [AgEagle](https://ageagle.com/solutions/micasense-series-multispectral-cameras/) and have been used for agricultural applications. We have experience using three sensors in our lab - RedEdge-3, Altum and RedEdge-P. They differ in the resolution of their cameras, but the three of them have five bands in common: Blue, Red, Green, Near Infrared and RedEdge. Altum has an extra thermal sensor, while RedEdge-P has a panchromatic camera.

The settings of a mission should be adjusted according to the sensor. We want a good frontal and side overlap for stitching images later, so we need to fly the drone in a way that we will guarantee these overlaps. We will basically control the number of paths on the flying app according to information like the size of the sensor, resolution, focal length, the triggering interval and the desired overlap. To start, let's take a look in the information of [Micasense Altum](https://support.micasense.com/hc/en-us/articles/360010025413-Altum-Integration-Guide) (serial number AL05) from the official website:

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0pky"></th>
    <th class="tg-0pky">Multispectral</th>
    <th class="tg-0pky">Thermal</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">Pixel size</td>
    <td class="tg-0pky">3.45 μm</td>
    <td class="tg-0pky">12 μm</td>
  </tr>
  <tr>
    <td class="tg-0pky">Resolution</td>
    <td class="tg-0pky">2064 x 1544 px  (3.2 MP x 5 imagers)</td>
    <td class="tg-0pky">160 x 120 px (0.01 K)</td>
  </tr>
  <tr>
    <td class="tg-0pky">Aspect ratio</td>
    <td class="tg-0pky">4 : 3</td>
    <td class="tg-0pky">4 : 3</td>
  </tr>
  <tr>
    <td class="tg-0pky">Sensor size</td>
    <td class="tg-0pky">7.12 x 5.33 mm  (8.9 mm diagonal)</td>
    <td class="tg-0pky">1.92 x 1.44 mm</td>
  </tr>
  <tr>
    <td class="tg-0pky">Focal length</td>
    <td class="tg-0pky">8 mm</td>
    <td class="tg-0pky">1.77 mm</td>
  </tr>
  <tr>
    <td class="tg-0pky">Field of view (h x v)</td>
    <td class="tg-0pky">48º x 36.8º</td>
    <td class="tg-0pky">57º x 44.3º</td>
  </tr>
  <tr>
    <td class="tg-0pky">Thermal sensitivity</td>
    <td class="tg-0pky">n/a</td>
    <td class="tg-0pky">&lt; 50 mK</td>
  </tr>
  <tr>
    <td class="tg-0pky">Thermal accuracy</td>
    <td class="tg-0pky">n/a</td>
    <td class="tg-0pky">+/- 5 K</td>
  </tr>
  <tr>
    <td class="tg-0pky">Output bit depth</td>
    <td class="tg-0pky">12-bit</td>
    <td class="tg-0pky">14-bit</td>
  </tr>
  <tr>
    <td class="tg-0pky">GSD @ 120 m (~400 ft)</td>
    <td class="tg-0pky">5.2 cm</td>
    <td class="tg-0pky">81 cm</td>
  </tr>
  <tr>
    <td class="tg-0pky">GSD @ 60 m (~200 ft)</td>
    <td class="tg-0pky">2.6 cm</td>
    <td class="tg-0pky">41 cm</td>
  </tr>
</tbody>
</table>


For example, we used the DJI GS Pro to perform our missions. In this app, we can enter a custom camera and the information we needed was the resolution, the sensor size (it's the size of each one of the five little cameras, not the structural size of the sensor) and focal length. See that we have two kinds of sensors, where thermal has less resolution than the multispectral ones. You need to choose which one of them helps you to achieve your research goals. For us, the objective was to use the data from the five multispectral band, so all our mission planning was focused on them.

 We configured the Altum to capture images in the overlap mode, asking for at least 85% overlap (do not forget to set up the target height). In the GS Pro app we set settings to 80% frontal and side overlap, keeping the speed of the drone around 3 to 4 mph. A very important point here is that you need to define the height of your missions! We had good experiences flying 90 and 50 ft, but there is no rule of thumb as it depends on the crop and objectives of the research. Choosing the height, increasing the overlap and controlling the speed might affect a lot the time of the flight, so you need to study how to optimize it for your conditions! A good advice is to play with your settings in test flights before you really start collecting the data. The good part is that after your mission is defined, you can use the same settings through the season!


## Check your images - FlightCheck

A good practice is to check your SD card after performing a mission, to be sure the images were saved. Unfortunately, our RedEdge-3 failed in some flights and, by checking our data, we could fly again the same field when we found problems. Well, for long missions, we know that it is a little boring to scan through all folders, especially because we have the same picture in multiple spectra. Also, sometimes there is no time to stitch the images and assess if the orthomosaic is complete. Imagine spending a good amount of time flying, stitching and getting a result like this in the end:

![]({{ site.baseurl}}{% link _posts/figures/20230901/ww_20230622_failed.png %}){:width="80%"}

Not ideal, right? It looks like a green Pac-Man or a [Rex](https://disney.fandom.com/wiki/Rex)! What happened in this flight is a very good example of a problem with the sensor. This was a long mission where we had to stop twice to change the battery, requiring us to turn off and on the drone. For some reason, when we took off the second time, RedEdge-3 did not save pictures in the SD card. To help with cases like these we created [FlightCheck](https://github.com/Lachowiec-Lab/FlightCheck), a very simple app where you can upload images and where pictures were taken in the field. It's possible to evaluate if you have all the paths as expected in the mission. There are two simple ways to use it: as a Shiny app on RStudio, or directly on the [website](https://correrfh.shinyapps.io/flightcheck/). The main difference is that in the first you can give the path of a folder with subfolders containing images; while in the latter you need to upload images to the website. For instructions please check the [FlightCheck](https://github.com/Lachowiec-Lab/FlightCheck) GitHub page.


# Stitching using Open Drone Map

We have been using open source tools in our lab for all the data processing. It would not be different for stitching! This task can be accomplished by [Open Drone Map](https://www.opendronemap.org), a very elegant effort to create an open ecosystem for drone image analysis! We have been using mostly the [command line toolkit](https://www.opendronemap.org/odm/) via [Docker](https://www.docker.com/) for stitching in the high performance computer. We also recommend installing [WebODM](https://www.opendronemap.org/webodm/) locally if you surveyed ground control points! The tutorials for installation are excellent, but feel free to contact us if you need some extra help!

**Let's get started!**

Here are tips from our experience stitching multispectral images. There are probably other ways to do some of the following steps!

## Multispectral images

Micasense sensors capture the bands in separate TIF files. Here are images for five bands (we did not include the thermal band from Altum):

<div class="card-columns">
    {% for img in page.images01 %}
    <div class="card">
        <img class="card-img-top" src="{{ img }}" />
    </div>
    {% endfor %}
</div>

To identify the pixels associated to the GCPs' coordinates, we will use WebODM. However, it only accepts PNG or JPG images. For that reason, we will choose one of the bands and convert it to PNG. On MacOS we can easily select images with a certain extension and convert them to the desired format. But we can make it simpler! We can install [ImageMagick](https://imagemagick.org/index.php) (search on Internet the easiest way to install it in your sytem) to use the function `mogrify` to convert TIF into JPG:

`mogrify -format jpg *_3.tif`

If you are in a folder with Micasense TIF images, this command will create JPG files from them. We used `*_3.tif` to select only the files finishing in `_3.tif`, as we only want images from the red band (in our experience, it was easier to find the central point of the GCP using red or NIR). 

### Note about folders

In a normal flight, Micasense saves a folder with multiple subfolders inside. If the SD card is empty, the first folder will be `0000SET`. If you turn the drone off and on again, it will create `0001SET`! Let's suppose that you turned on the drone, took pictures of the reflectance panel, turned it off, positioned it and turned it on for the mission. This will make you have `0000SET` containing a `000` subfolder with images of the panel, and `0001SET` with multiple subfolders (`000`,`001`,`0002`,...) with the images of the flight. You would use the images inside the `0001SET` for stitching. It will be easy because their identifiers **will not be repeated** and it will be essential for ODM later. Please take a look to be sure the identifiers in the subfolders are not the same!!!

For flights where you need to change your battery, multiple SET folders will be used - `0000SET`, `0001SET` and so on. The problem here is that now the images in the subfolders have the same identifiers. Suppose that we started the mission and images were saved to `0000SET`, we stopped the drone to change its battery and continued from the point where we started so that a new folder `0001SET` will be created. The first image on `0000SET/000/` is `IMG_0000_1.tif` (`_1.tif` indicates it is the first band) and the first image on `0001SET/000/` is also `IMG_0000_1.tif`! If we do not rename the identifiers, it can cause a big problem during stitching when you put all the images in the same folder (one set will be overwritten by the other). Avoid it by renaming properly!

## Identifying the GCPs

After the surveying the GCP, the data needs to be processed, and you need to identify some important information:

- The coordinate system and map projection (usually a `.prj`) file. It needs to be formatted and included as the first line of the GCP file that will be used for stitching. For more information please take a look at this [guideline](https://docs.opendronemap.org/gcp/).
- A spatial index file (`.sbn`) containing the GCP name and information about Northing, Easting and elevation.

You then join these two information into a file. Something similar to this file:

```
+proj=utm +zone=12 +ellps=GRS80 +units=m +no_defs
gcp01 488285.320749235 5057952.81667286 1449.048
gcp02 488286.209505209 5057908.07579038 1449.545
gcp03 488334.427102019 5057907.89432212 1449.789
gcp04 488392.786749728 5057908.04653477 1450.277
gcp05 488393.97363847 5057952.79020276 1449.542
gcp06 488333.696648627 5057951.67609977 1449.311
gcp07 488347.239315505 5057927.95963317 1449.68399999999
gcp08 488316.984131151 5057927.95734618 1449.39200000001
```

Here is a short video you load the images, the GCP data and associate them:

<iframe width="480" height="360" src="/fhc-www/_posts/figures/20230901/WebODM_GCP.mp4" frameborder="0"> </iframe>

Try to do that for multiple images of the same GCP (at least three). When you finish, you can use the button in the upper right corner to export the new file, where pixel coordinates in images are associated to geographic coordinates. Save it to use during stitching!

## Open Drone Map (ODM)

You can use WebODM for stitching! However, the ODM Docker container is pretty handy when you need more computational resources. Some systems might not have Docker, but singularity instead. In this case, the image needs to be saved as an `.sif` file pulling the image directly from Docker:

`singularity pull docker://opendronemap/odm`

This will create a file named `odm_latest.sif` that can be used with `singularity`.

When stitching images using Open Drone Map (ODM), we have been using the following command for images captured by Micasense sensors (RedEdge 3, RedEdge-P and Altum):

```
singularity run \
    --bind /path_to_image_dir/:/path_to_output_directories/code/images
    --writable-tmpsf odm_latest.sif \ 
    --gcp gcp_file.txt --dem-resolution 1.18 --orthophoto-resolution 1.18 \
    --orthophoto-png --dsm \
    --radiometric-calibration camera --texturing-data-term area --texturing-skip-global-seam-leveling \
    --pc-quality ultra 
    --project-path /path_to_output_directories/
```

* The first line will "bind" the folder where images are located to the container folder. Note that the last line has a local project path to an output folder.

* Second line the container to be used by singularity

* Lines one to four have arguments that can be used by ODM to perform the stitching process:

    * gcp : give the path to your GCP file

    * dem-resolution and orthophoto-resoltion: DSM/DTM and orthophoto resolution

    * dsm: build a Digital Surface Model (DSM)
    
    * radiometric-calibration: ODM has two calibration `camera` or `camera+sun`, and as the second still experimental we then decided to use only `camera`

    * texturing-data-term: When texturing the 3D mesh, the priority are images that cover the larger area
    
    * texturing-skip-global-seam-leveling: We skip the normalization of colors across all images (we are dealing with radiometric data, so this step can be skipped)

    * pc-quality: This argument sets the point cloud quality

You can ask: where did we get the values for resolution? It is based on the Ground Sampling Distance (GSD) can be defined as the distance between pixel centers measured on the ground. How can we obtain this value? It depends mostly on the camera and the height of the flight, and AgEagle provides us this value! There is a [pre-flight calculator](https://ageagle.com/calculator/) for the MicaSense series. You choose the model and the type of the camera, enter the altitude, and it will show the GSD value!

Here is one example of stitched images of a field, where mission was repeated three times. As we positioned the GCPs, surveyed them and used their coordinates, they can be easily compared:

![]({{ site.baseurl}}{% link _posts/figures/20230901/pmaps_gif_deconstruct_2000.gif %}){:width="80%"}

# Drone Image Analysis Pipeline

UNDER CONSTRUCTION